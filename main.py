import json
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.calibration import CalibratedClassifierCV
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup
from torch.optim import AdamW
import re
from tqdm import tqdm
import nltk
from nltk.stem import WordNetLemmatizer
import gc
import warnings

# å¿½ç•¥ä¸å¿…è¦çš„è­¦å‘Š
warnings.filterwarnings("ignore")

# ==========================================
# 0. é«˜ç«¯é…ç½® (ç¨³å¥ç‰ˆ)
# ==========================================
try:
    nltk.data.find('corpora/wordnet')
except LookupError:
    nltk.download('wordnet')
    nltk.download('omw-1.4')

CONFIG = {
    'seed': 2024,
    'model_name': 'microsoft/deberta-v3-large', 
    'max_len': 128,
    
    # æ˜¾å­˜/ç¨³å®šæ€§ä¼˜åŒ–é…ç½® (ä½ çš„ä¿®æ”¹ç‰ˆ)
    'batch_size': 2,        
    'accum_steps': 16,        
    'use_checkpointing': False, 
    
    'epochs': 10, # æœ‰äº†éªŒè¯é›†ï¼Œæˆ‘ä»¬å¯ä»¥å°‘è·‘å‡ è½®ï¼Œè§‚å¯Ÿæ”¶æ•›å³å¯
    'lr': 1e-5,              
    'device': 'cuda' if torch.cuda.is_available() else 'cpu',
    'pseudo_threshold': 0.95 
}

def set_seed(seed):
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True

set_seed(CONFIG['seed'])
print(f"ğŸš€ Running IMPROVED solution on: {CONFIG['device']} with {CONFIG['model_name']}")

# ==========================================
# 1. å¢å¼ºç‰ˆæ•°æ®æ¸…æ´— (Data Cleaning)
# ==========================================
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    text = text.lower()
    
    # [ä¿®æ”¹1] å°†è¿å­—ç¬¦æ›¿æ¢ä¸ºç©ºæ ¼ (low-fat -> low fat)ï¼Œæ–¹ä¾¿å»é™¤ split åçš„å•è¯
    text = text.replace('-', ' ')
    
    # [ä¿®æ”¹2] ä»…ä¿ç•™å­—æ¯å’Œç©ºæ ¼
    text = re.sub(r'[^a-z\s]', '', text)
    
    # [ä¿®æ”¹3] å¤§å¹…æ‰©å……çš„åœç”¨è¯åº“ (åŸºäºå¯¹é£Ÿææ•°æ®çš„æ·±å…¥åˆ†æ)
    stop_words = set([
        # --- åŸå§‹åˆ—è¡¨ ---
        'fresh', 'ground', 'chopped', 'sliced', 'diced', 'crushed', 'minced', 'grated', 
        'large', 'medium', 'small', 'cloves', 'lb', 'oz', 'drained', 'pitted', 'beaten', 
        'unsalted', 'all-purpose', 'chunks', 'dried', 'leaves', 'powder', 'frozen', 'warm',
        'melted', 'boneless', 'skinless', 'halves', 'raw', 'extra', 'virgin',
        
        # --- æ–°å¢ï¼šåŠ å·¥çŠ¶æ€ ---
        'canned', 'jarred', 'stewed', 'condensed', 'evaporated', 'thawed', 'smoked',
        'cured', 'pickled', 'harden', 'softened', 'puree', 'paste',
        
        # --- æ–°å¢ï¼šå½¢çŠ¶/åˆ‡å‰² ---
        'cubed', 'wedges', 'strips', 'rings', 'lengthwise', 'pieces', 'segments', 
        'florets', 'spears', 'hearts', 'whole', 'fillet', 'filet', 'loins',
        
        # --- æ–°å¢ï¼šå¥åº·/æˆåˆ†æ ‡ç­¾ ---
        'low', 'fat', 'nonfat', 'free', 'reduced', 'sodium', 'gluten', 'skim', 'part',
        'light', 'lite', 'organic',
        
        # --- æ–°å¢ï¼šæ¸©åº¦/ç‰©ç† ---
        'room', 'temperature', 'lukewarm', 'cold', 'hot', 'boiling',
        
        # --- æ–°å¢ï¼šé€šç”¨é‡è¯/å®¹å™¨ ---
        'cup', 'cups', 'teaspoon', 'tablespoon', 'tbsp', 'tsp', 'pinch', 'dash',
        'quart', 'pint', 'gallon', 'bottle', 'can', 'stick', 'pack', 'package'
    ])
    
    words = [w for w in text.split() if w not in stop_words]
    words = [lemmatizer.lemmatize(w) for w in words]
    
    # é‡æ–°ç»„åˆ
    cleaned = ' '.join(words)
    
    # [å…œåº•ç­–ç•¥] å¦‚æœæ¸…æ´—åæŠŠè¯éƒ½æ´—æ²¡äº† (æ¯”å¦‚ "fresh large"), å°±è¿”å›åŸè¯ï¼Œé˜²æ­¢ç©ºå­—ç¬¦ä¸²
    if not cleaned.strip():
        return text
        
    return cleaned

def load_data():
    print("Loading data...")
    with open('train.json', 'r', encoding='utf-8') as f:
        train_data = json.load(f)
    with open('test.json', 'r', encoding='utf-8') as f:
        test_data = json.load(f)
        
    train_df = pd.DataFrame(train_data)
    test_df = pd.DataFrame(test_data)
    
    print("Cleaning ingredients with ENHANCED rules...")
    train_df['clean_list'] = train_df['ingredients'].apply(lambda x: [clean_text(i) for i in x])
    test_df['clean_list'] = test_df['ingredients'].apply(lambda x: [clean_text(i) for i in x])
    
    train_df['text_str'] = train_df['clean_list'].apply(lambda x: ', '.join(x)) 
    test_df['text_str'] = test_df['clean_list'].apply(lambda x: ', '.join(x))
    
    return train_df, test_df

# ==========================================
# 2. æ¨¡å‹ A: LinearSVC
# ==========================================
def train_svc(X_train, y_train, X_val, y_val, X_test):
    print("\n[LinearSVC] Training...")
    # TF-IDF è®¾ç½®
    tfidf = TfidfVectorizer(binary=True, ngram_range=(1, 2), min_df=3, max_df=0.9, sublinear_tf=True)
    
    # æ‹Ÿåˆè®­ç»ƒé›†
    X_train_vec = tfidf.fit_transform(X_train)
    X_val_vec = tfidf.transform(X_val)
    X_test_vec = tfidf.transform(X_test)
    
    svc = LinearSVC(C=0.6, penalty='l2', dual=False, max_iter=3000, random_state=CONFIG['seed'])
    clf = CalibratedClassifierCV(svc, method='sigmoid', cv=5)
    
    clf.fit(X_train_vec, y_train)
    
    # æ‰“å°éªŒè¯é›†åˆ†æ•°
    val_score = clf.score(X_val_vec, y_val)
    print(f"[LinearSVC] Validation Accuracy: {val_score:.4f}")
    
    probs = clf.predict_proba(X_test_vec)
    return probs, clf, tfidf

# ==========================================
# 3. æ¨¡å‹ B: DeBERTa-v3-Large (å«éªŒè¯å¾ªç¯)
# ==========================================
class CuisineDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len, augment=False):
        self.texts = texts 
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.augment = augment
        
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        ingredients = self.texts[idx]
        if self.augment:
            ingredients = list(ingredients)
            np.random.shuffle(ingredients)
            
        text = ", ".join(ingredients)
        
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt',
        )
        
        item = {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten()
        }
        
        if self.labels is not None:
            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)
            
        return item

def train_transformer(X_train, y_train, X_val, y_val, X_test, num_classes, pseudo_texts=None, pseudo_labels=None):
    print(f"\n[Transformer] Training {CONFIG['model_name']}...")
    
    tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])
    
    # åˆå¹¶ä¼ªæ ‡ç­¾æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰åˆ°è®­ç»ƒé›†
    all_train_texts = list(X_train)
    all_train_labels = list(y_train)
    
    if pseudo_texts is not None:
        print(f"ğŸ”¥ Adding {len(pseudo_texts)} Pseudo-Labeled samples to training!")
        all_train_texts.extend(pseudo_texts)
        all_train_labels.extend(pseudo_labels)
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¸æŠŠä¼ªæ ‡ç­¾åŠ åˆ°éªŒè¯é›†ï¼ŒéªŒè¯é›†å¿…é¡»ä¿æŒçº¯å‡€
    
    # æ„å»º Dataset
    train_dataset = CuisineDataset(all_train_texts, all_train_labels, tokenizer, CONFIG['max_len'], augment=True)
    val_dataset = CuisineDataset(X_val, y_val, tokenizer, CONFIG['max_len'], augment=False)
    test_dataset = CuisineDataset(X_test, None, tokenizer, CONFIG['max_len'], augment=False)
    
    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=0, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size']*2, shuffle=False, num_workers=0) # éªŒè¯é›†BSå¯ä»¥å¤§ä¸€ç‚¹
    test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size']*2, shuffle=False, num_workers=0)
    
    model = AutoModelForSequenceClassification.from_pretrained(CONFIG['model_name'], num_labels=num_classes)
    model.to(CONFIG['device'])
    
    # æ˜¾å­˜ä¼˜åŒ–é…ç½®
    if CONFIG['use_checkpointing']:
        model.config.use_cache = False
        model.gradient_checkpointing_enable()
    
    optimizer = AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=0.01)
    total_steps = len(train_loader) * CONFIG['epochs'] // CONFIG['accum_steps']
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(total_steps*0.1), num_training_steps=total_steps)
    
    scaler = torch.amp.GradScaler('cuda') 
    loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)

    # --- Training Loop ---
    best_val_acc = 0.0
    
    for epoch in range(CONFIG['epochs']):
        model.train()
        total_train_loss = 0
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{CONFIG['epochs']}")
        
        for step, batch in enumerate(pbar):
            input_ids = batch['input_ids'].to(CONFIG['device'])
            attention_mask = batch['attention_mask'].to(CONFIG['device'])
            labels = batch['labels'].to(CONFIG['device'])
            
            with torch.amp.autocast('cuda'): 
                outputs = model(input_ids, attention_mask=attention_mask)
                loss = loss_fn(outputs.logits, labels)
                loss = loss / CONFIG['accum_steps'] 
            
            scaler.scale(loss).backward()
            
            if (step + 1) % CONFIG['accum_steps'] == 0:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                scaler.step(optimizer)
                scaler.update()
                scheduler.step()
                optimizer.zero_grad()
            
            total_train_loss += loss.item() * CONFIG['accum_steps']
            pbar.set_postfix({'loss': total_train_loss / (step + 1)})
        
        # --- Validation Loop (æ¯ä¸ª Epoch ç»“æŸåè¿è¡Œ) ---
        model.eval()
        val_loss = 0
        correct = 0
        total = 0
        
        print(f"Validating Epoch {epoch+1}...")
        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(CONFIG['device'])
                attention_mask = batch['attention_mask'].to(CONFIG['device'])
                labels = batch['labels'].to(CONFIG['device'])
                
                with torch.amp.autocast('cuda'):
                    outputs = model(input_ids, attention_mask=attention_mask)
                    loss = loss_fn(outputs.logits, labels)
                
                val_loss += loss.item()
                _, predicted = torch.max(outputs.logits, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        
        avg_val_loss = val_loss / len(val_loader)
        val_acc = correct / total
        print(f"ğŸ‘‰ [Epoch {epoch+1}] Train Loss: {total_train_loss/len(train_loader):.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}")
        
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            # åœ¨è¿™é‡Œå¯ä»¥ä¿å­˜æ¨¡å‹ï¼Œæˆ–è€…ç›´æ¥ç»§ç»­è·‘
            
    print("Generating predictions...")
    model.eval()
    all_probs = []
    
    with torch.no_grad():
        for batch in tqdm(test_loader):
            input_ids = batch['input_ids'].to(CONFIG['device'])
            attention_mask = batch['attention_mask'].to(CONFIG['device'])
            
            with torch.amp.autocast('cuda'):
                outputs = model(input_ids, attention_mask=attention_mask)
            
            probs = torch.softmax(outputs.logits, dim=1).float()
            all_probs.append(probs.cpu().numpy())
            
    del model, optimizer, scaler
    torch.cuda.empty_cache()
    gc.collect()
    
    return np.concatenate(all_probs, axis=0)

# ==========================================
# 4. ä¸»æµç¨‹ (å«ä¼ªæ ‡ç­¾)
# ==========================================
def main():
    train_df, test_df = load_data()
    
    le = LabelEncoder()
    # å…¨é‡æ ‡ç­¾ç¼–ç 
    y_full = le.fit_transform(train_df['cuisine'])
    num_classes = len(le.classes_)
    
    # [æ–°å¢] åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›† (90% Train, 10% Val)
    # ä½¿ç”¨ stratify ä¿è¯æ¯ä¸€ç±»èœç³»çš„æ¯”ä¾‹åœ¨éªŒè¯é›†ä¸­ä¸€è‡´
    print("Splitting data into Train (90%) and Validation (10%)...")
    X_train_raw, X_val_raw, y_train, y_val = train_test_split(
        train_df['clean_list'].values, 
        y_full, 
        test_size=0.1, 
        random_state=CONFIG['seed'], 
        stratify=y_full
    )
    
    # ä¸ºäº† SVC éœ€è¦ string æ ¼å¼
    X_train_str = [", ".join(x) for x in X_train_raw]
    X_val_str = [", ".join(x) for x in X_val_raw]
    X_test_str = test_df['text_str'].values
    
    # --- Round 1: åˆå§‹è®­ç»ƒ ---
    print("\n" + "="*30 + "\n ROUND 1: Initial Training \n" + "="*30)
    
    # è®­ç»ƒ SVC (å¸¦éªŒè¯)
    svc_probs, svc_model, tfidf_model = train_svc(X_train_str, y_train, X_val_str, y_val, X_test_str)
    
    # è®­ç»ƒ DeBERTa (å¸¦éªŒè¯)
    deb_probs = train_transformer(X_train_raw, y_train, X_val_raw, y_val, test_df['clean_list'].values, num_classes)
    
    ensemble_probs_r1 = (svc_probs * 0.4) + (deb_probs * 0.6) 
    
    # --- Round 2: ä¼ªæ ‡ç­¾ (Pseudo Labeling) ---
    print("\n" + "="*30 + "\n ROUND 2: Pseudo Labeling \n" + "="*30)
    
    max_probs = np.max(ensemble_probs_r1, axis=1)
    pseudo_indices = np.where(max_probs >= CONFIG['pseudo_threshold'])[0]
    pseudo_labels = np.argmax(ensemble_probs_r1[pseudo_indices], axis=1)
    
    print(f"Found {len(pseudo_indices)} samples with confidence >= {CONFIG['pseudo_threshold']}")
    
    if len(pseudo_indices) > 0:
        pseudo_texts_list = test_df['clean_list'].iloc[pseudo_indices].values
        pseudo_texts_str = test_df['text_str'].iloc[pseudo_indices].values
        
        # é‡æ–°è®­ç»ƒ SVC (Train + Pseudo)
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¾ç„¶åœ¨åŸå§‹ Val ä¸ŠéªŒè¯ï¼Œçœ‹çœ‹åŠ å…¥ä¼ªæ ‡ç­¾åæ¨¡å‹æ˜¯å¦å˜å¼º
        X_train_full_str = X_train_str + list(pseudo_texts_str)
        y_train_full = np.concatenate([y_train, pseudo_labels])
        
        svc_probs_r2, _, _ = train_svc(X_train_full_str, y_train_full, X_val_str, y_val, X_test_str)
        
        # é‡æ–°è®­ç»ƒ DeBERTa
        deb_probs_r2 = train_transformer(
            X_train_raw, y_train, 
            X_val_raw, y_val, # éªŒè¯é›†ä¸å˜
            test_df['clean_list'].values, num_classes,
            pseudo_texts=pseudo_texts_list, pseudo_labels=pseudo_labels
        )
        
        final_probs = (svc_probs_r2 * 0.4) + (deb_probs_r2 * 0.6)
    else:
        print("Not enough confident samples for pseudo labeling. Using Round 1 results.")
        final_probs = ensemble_probs_r1

    final_preds = np.argmax(final_probs, axis=1)
    final_labels = le.inverse_transform(final_preds)
    
    submission = pd.DataFrame({
        'id': test_df['id'],
        'cuisine': final_labels
    })
    submission.to_csv('submission_sota.csv', index=False)
    print("Done! Check submission_sota.csv")

if __name__ == '__main__':
    main()